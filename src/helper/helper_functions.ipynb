{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### **Note:** \n",
    "#### Be careful, all the files in input_directory will be deleted after the execution of the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries, load dependencies and defining variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # Import os library for file and directory operations\n",
    "import zipfile  # Import zipfile library for handling zip files\n",
    "import glob  # Import glob library for file pattern matching\n",
    "import shutil  # Import shutil library for file operations\n",
    "import pandas as pd  # Import pandas library for data manipulation\n",
    "\n",
    "input_dir = 'input'  # Define the input directory\n",
    "output_dir = 'output'  # Define the output directory\n",
    "\n",
    "CSV_PATH = \"../../data/csv\"  # Define the path to the CSV files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_update_status(row):\n",
    "    variant = row[\"variant\"]  # Get the variant from the row\n",
    "    filename = variant.replace('_p.', '_')  # Replace '_p.' with '_' in the variant name\n",
    "    file_path = f\"{PDB_PATH}/{filename}.pdb\"  # Define the file path for the PDB file\n",
    "    print(file_path)  # Print the file path\n",
    "    if os.path.isfile(file_path):  # Check if the PDB file exists\n",
    "        return 'concluded'  # Return 'concluded' if the file exists\n",
    "    return 'not_concluded'  # Return 'not_concluded' if the file does not exist\n",
    "\n",
    "def update_status(df, column):\n",
    "    if column not in df.columns:  # Check if the column is not in the DataFrame\n",
    "        df[column] = 'not_concluded'  # Add the column with default value 'not_concluded'\n",
    "    df[column] = df.apply(check_and_update_status, axis=1)  # Update the status for each row\n",
    "    print(\"Status updated based on existing files\")  # Print a status update message\n",
    "    return df  # Return the updated DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Swiss Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update .csv with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"../../data/pdb/swiss_model\"  # Define the path to the Swiss Model PDB files\n",
    "df = pd.read_csv(f'{CSV_PATH}/fasta_variant.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "df = update_status(df, \"swiss_model\")  # Update the Swiss Model status in the DataFrame\n",
    "print(df['swiss_model'].value_counts())  # Count the values in the Swiss Model column\n",
    "df.to_csv(f'{CSV_PATH}/fasta_variant.csv', index=False, sep=';')  # Save the updated DataFrame to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Colab AlphaFold2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Extract pdb from colab alphafold2 zip files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = '../../data/pdb/colab_alphafold2'  # Define the extraction directory\n",
    "\n",
    "os.makedirs(extract_dir, exist_ok=True)  # Create the extraction directory if it does not exist\n",
    "\n",
    "zip_files = glob.glob(os.path.join(input_dir, '*.zip'))  # Get the list of zip files in the input directory\n",
    "\n",
    "for zip_file in zip_files:  # Iterate over the zip files\n",
    "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:  # Open the zip file\n",
    "        temp_dir = os.path.join(input_dir, 'temp')  # Define the temporary directory\n",
    "        os.makedirs(temp_dir, exist_ok=True)  # Create the temporary directory\n",
    "        zip_ref.extractall(temp_dir)  # Extract the zip file to the temporary directory\n",
    "        for root, _, files in os.walk(temp_dir):  # Iterate over the files in the temporary directory\n",
    "            for file in files:  # Iterate over the files\n",
    "                if file.endswith('.pdb'):  # Check if the file is a PDB file\n",
    "                    dest_path = os.path.join(extract_dir, file)  # Define the destination path\n",
    "                    if not os.path.exists(dest_path):  # Check if the destination file does not exist\n",
    "                        shutil.move(os.path.join(root, file), dest_path)  # Move the PDB file to the destination path\n",
    "            \n",
    "        shutil.rmtree(temp_dir)  # Remove the temporary directory\n",
    "\n",
    "    os.remove(zip_file)  # Remove the zip file\n",
    "\n",
    "print(\"Extraction complete.\")  # Print a completion message"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update .csv with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"../../data/pdb/colab_alphafold2\"  # Define the path to the Colab AlphaFold2 PDB files\n",
    "df = pd.read_csv(f'{CSV_PATH}/fasta_variant.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "df = update_status(df, \"colab_alphafold2\")  # Update the Colab AlphaFold2 status in the DataFrame\n",
    "print(df['colab_alphafold2'].value_counts())  # Count the values in the Colab AlphaFold2 column\n",
    "df.to_csv(f'{CSV_PATH}/fasta_variant.csv', index=False, sep=';')  # Save the updated DataFrame to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phyre2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract fasta files to use as input on Phyre2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_fasta_files(input_dir, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it does not exist\n",
    "    fasta_files = glob.glob(os.path.join(input_dir, '**/*.fasta'), recursive=True)  # Get the list of FASTA files\n",
    "    \n",
    "    concatenated_content = []  # Initialize a list to store concatenated content\n",
    "    file_count = 0  # Initialize a file count\n",
    "    variant_index = 1  # Initialize a variant index\n",
    "    start_index = 1  # Initialize a start index\n",
    "\n",
    "    for fasta_file in fasta_files:  # Iterate over the FASTA files\n",
    "        with open(fasta_file, 'r') as f:  # Open the FASTA file for reading\n",
    "            lines = f.readlines()  # Read the lines from the FASTA file\n",
    "            concatenated_content.extend(lines)  # Extend the concatenated content with the lines\n",
    "            concatenated_content.append('\\n')  # Add a newline character after each file\n",
    "            file_count += 1  # Increment the file count\n",
    "\n",
    "            if file_count == 100:  # Check if the file count is 100\n",
    "                end_index = start_index + file_count - 1  # Calculate the end index\n",
    "                output_file = os.path.join(output_dir, f'variant_{start_index}_{end_index}.fasta')  # Define the output file path\n",
    "                with open(output_file, 'w') as out_f:  # Open the output file for writing\n",
    "                    out_f.writelines(concatenated_content)  # Write the concatenated content to the output file\n",
    "        \n",
    "                concatenated_content = []  # Reset the concatenated content\n",
    "                file_count = 0  # Reset the file count\n",
    "                start_index = end_index + 1  # Update the start index\n",
    "                variant_index += 1  # Increment the variant index\n",
    "\n",
    "        os.remove(fasta_file)  # Remove the FASTA file\n",
    "\n",
    "    if concatenated_content:  # Check if there is remaining concatenated content\n",
    "        end_index = start_index + file_count - 1  # Calculate the end index\n",
    "        output_file = os.path.join(output_dir, f'variant_{start_index}_{end_index}.fasta')  # Define the output file path\n",
    "        with open(output_file, 'w') as out_f:  # Open the output file for writing\n",
    "            out_f.writelines(concatenated_content)  # Write the concatenated content to the output file\n",
    "\n",
    "    print(\"Extraction complete.\")  # Print a completion message\n",
    "    shutil.rmtree(input_dir)  # Remove the input directory\n",
    "\n",
    "extract_fasta_files(input_dir, output_dir)  # Call the function to extract FASTA files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract PDB files from Phyre2 result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdb_files_from_phyre2(input_dir, csv_path, output_dir):\n",
    "    summary_dict = {}  # Initialize a dictionary to store summary information\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)  # Create the output directory if it does not exist\n",
    "\n",
    "    zip_files = glob.glob(os.path.join(input_dir, '*.zip'))  # Get the list of zip files in the input directory\n",
    "    for zip_file in zip_files:  # Iterate over the zip files\n",
    "        with zipfile.ZipFile(zip_file, 'r') as zip_ref:  # Open the zip file\n",
    "            temp_dir = os.path.join(input_dir, 'temp')  # Define the temporary directory\n",
    "            os.makedirs(temp_dir, exist_ok=True)  # Create the temporary directory\n",
    "            zip_ref.extractall(temp_dir)  # Extract the zip file to the temporary directory\n",
    "\n",
    "            summaryinfo_path = os.path.join(temp_dir, 'summaryinfo')  # Define the path to the summaryinfo file\n",
    "            if os.path.isfile(summaryinfo_path):  # Check if the summaryinfo file exists\n",
    "                with open(summaryinfo_path, 'r') as summary_file:  # Open the summaryinfo file for reading\n",
    "                    for line in summary_file:  # Iterate over the lines in the summaryinfo file\n",
    "                        if line.startswith('#'):  # Skip lines that start with '#'\n",
    "                            continue\n",
    "                        parts = line.strip().split('|')  # Split the line into parts\n",
    "                        if len(parts) >= 4:  # Check if there are at least 4 parts\n",
    "                            description = f\"{parts[1].strip()}_{parts[2].strip()}\"  # Create a description from parts 1 and 2\n",
    "                            sequence_identity = parts[3].strip()  # Get the sequence identity from part 3\n",
    "                            summary_dict[sequence_identity] = description  # Add the sequence identity and description to the dictionary\n",
    "\n",
    "            for pdb_file in glob.glob(os.path.join(temp_dir, '*.final.pdb')):  # Iterate over the PDB files\n",
    "                pdb_filename = os.path.basename(pdb_file)  # Get the base name of the PDB file\n",
    "                pdb_key = os.path.splitext(pdb_filename)[0].split(\".\")[0]  # Get the key from the PDB file name\n",
    "                if pdb_key in summary_dict:  # Check if the key is in the summary dictionary\n",
    "                    description = summary_dict[pdb_key]  # Get the description from the summary dictionary\n",
    "                    value1, value2 = description.split('_')  # Split the description into value1 and value2\n",
    "                    csv_file_path = os.path.join(csv_path, 'fasta_variant.csv')  # Define the path to the CSV file\n",
    "                    df = pd.read_csv(csv_file_path, sep=';')  # Read the CSV file into a DataFrame\n",
    "                    gene = df.loc[df['identifier'] == value1, 'gene'].values[0]  # Get the gene from the DataFrame\n",
    "                    new_pdb_filename = f\"{gene}_{value2}.pdb\"  # Create a new PDB file name\n",
    "                    new_pdb_path = os.path.join(output_dir, new_pdb_filename)  # Define the new PDB file path\n",
    "                    shutil.move(pdb_file, new_pdb_path)  # Move the PDB file to the new PDB path\n",
    "\n",
    "            shutil.rmtree(temp_dir)  # Remove the temporary directory\n",
    "        os.remove(zip_file)  # Remove the zip file\n",
    "\n",
    "    csv_file_path = os.path.join(csv_path, 'fasta_variant.csv')  # Define the path to the CSV file\n",
    "    df = pd.read_csv(csv_file_path, sep=';')  # Read the CSV file into a DataFrame\n",
    "\n",
    "    for sequence_identity, description in summary_dict.items():  # Iterate over the summary dictionary\n",
    "        value1, value2 = description.split('_')  # Split the description into value1 and value2\n",
    "        gene = df.loc[df['identifier'] == value1, 'gene'].values[0]  # Get the gene from the DataFrame\n",
    "        new_description = f\"{gene}_{value2}\"  # Create a new description\n",
    "        if os.path.isfile(os.path.join(output_dir, f\"{new_description}.pdb\")):  # Check if the PDB file exists\n",
    "            df.loc[df['identifier'] == value1, 'phyre2'] = 'concluded'  # Update the status to 'concluded'\n",
    "        else:\n",
    "            df.loc[df['identifier'] == value1, 'phyre2'] = 'not_concluded'  # Update the status to 'not_concluded'\n",
    "\n",
    "    df.to_csv(csv_file_path, sep=';', index=False)  # Save the updated DataFrame to CSV\n",
    "\n",
    "    return summary_dict  # Return the summary dictionary\n",
    "\n",
    "input_dir = 'input'  # Define the input directory\n",
    "csv_path = '../../data/csv'  # Define the path to the CSV files\n",
    "output_dir = '../../data/pdb/phyre2'  # Define the output directory\n",
    "\n",
    "summary_dict = extract_pdb_files_from_phyre2(input_dir, csv_path, output_dir)  # Call the function to extract PDB files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update .csv with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"../../data/pdb/phyre2\"  # Define the path to the Phyre2 PDB files\n",
    "df = pd.read_csv(f'{CSV_PATH}/fasta_variant.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "df = update_status(df, \"phyre2\")  # Update the Phyre2 status in the DataFrame\n",
    "print(df['phyre2'].value_counts())  # Count the values in the Phyre2 column\n",
    "df.to_csv(f'{CSV_PATH}/fasta_variant.csv', index=False, sep=';')  # Save the updated DataFrame to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update blast column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_and_update_blast_status(row):\n",
    "    gene = row[\"gene\"]  # Get the gene from the row\n",
    "    file_path = f\"{PDB_PATH}/{gene}.pdb\"  # Define the file path for the PDB file\n",
    "    print(file_path)  # Print the file path\n",
    "    if os.path.isfile(file_path):  # Check if the PDB file exists\n",
    "        return 'concluded'  # Return 'concluded' if the file exists\n",
    "    return 'not_concluded'  # Return 'not_concluded' if the file does not exist\n",
    "\n",
    "PDB_PATH = \"../../data/pdb/blast\"  # Define the path to the Blast PDB files\n",
    "df = pd.read_csv(f'{CSV_PATH}/fasta_wild.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "if \"blast\" not in df.columns:  # Check if the 'blast' column is not in the DataFrame\n",
    "    df[\"blast\"] = 'not_concluded'  # Add the 'blast' column with default value 'not_concluded'\n",
    "df[\"blast\"] = df.apply(check_and_update_blast_status, axis=1)  # Update the Blast status for each row\n",
    "print(\"Status updated based on existing files\")  # Print a status update message\n",
    "print(df['blast'].value_counts())  # Count the values in the Blast column\n",
    "\n",
    "df.to_csv(f'{CSV_PATH}/fasta_wild.csv', index=False, sep=';')  # Save the updated DataFrame to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaFold3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdb_files():\n",
    "    destination_folder = os.path.abspath(\"../../data/pdb/alphafold3\")  # Define the destination folder\n",
    "    os.makedirs(destination_folder, exist_ok=True)  # Create the destination folder if it does not exist\n",
    "    pdb_files = glob.glob(os.path.join(input_dir, \"*.pdb\"))  # Get the list of PDB files in the input directory\n",
    "    for pdb_file in pdb_files:  # Iterate over the PDB files\n",
    "        new_filename = os.path.basename(pdb_file).replace(\"_p.\", \"_\").replace(\"_model0\", \"\")  # Create a new file name\n",
    "        destination_file = os.path.join(destination_folder, new_filename)  # Define the destination file path\n",
    "\n",
    "        shutil.move(pdb_file, destination_file)  # Move the PDB file to the destination path\n",
    "        print(f\"Moved file: {pdb_file} to {destination_file}\")  # Print a success message\n",
    "    all_files = glob.glob(os.path.join(input_dir, \"*.*\"))  # Get the list of all files in the input directory\n",
    "    for file in all_files:  # Iterate over the files\n",
    "        if not file.endswith(\".pdb\"):  # Check if the file is not a PDB file\n",
    "            os.remove(file)  # Remove the file\n",
    "            print(f\"Deleted file: {file}\")  # Print a success message\n",
    "\n",
    "process_pdb_files()  # Call the function to process PDB files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update .csv with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"../../data/pdb/alphafold3\"  # Define the path to the AlphaFold3 PDB files\n",
    "df = pd.read_csv(f'{CSV_PATH}/fasta_variant.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "df = update_status(df, \"alphafold3\")  # Update the AlphaFold3 status in the DataFrame\n",
    "print(df['alphafold3'].value_counts())  # Count the values in the AlphaFold3 column\n",
    "df.to_csv(f'{CSV_PATH}/fasta_variant.csv', index=False, sep=';')  # Save the updated DataFrame to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update .csv with files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Update .csv with files\n",
    "PDB_PATH = \"../../data/pdb/modeller\"  # Define the path to the Modeller PDB files\n",
    "df = pd.read_csv(f'{CSV_PATH}/fasta_variant.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "df = update_status(df, \"modeller\")  # Update the Modeller status in the DataFrame\n",
    "print(df['modeller'].value_counts())  # Count the values in the Modeller column\n",
    "df.to_csv(f'{CSV_PATH}/fasta_variant.csv', index=False, sep=';')  # Save the updated DataFrame to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_files(path):\n",
    "    for root, dirs, files in os.walk(path):  # Walk through the directory\n",
    "        for file in files:  # Iterate over the files\n",
    "            new_filename = file.replace(\"_p.\", \"_\")  # Create a new file name\n",
    "            os.rename(os.path.join(root, file), os.path.join(root, new_filename))  # Rename the file\n",
    "            print(f\"Renamed file: {file} to {new_filename}\")  # Print a success message\n",
    "\n",
    "rename_files(\"../../data\")  # Call the function to rename files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
