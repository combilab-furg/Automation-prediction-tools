{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Note:** \n",
    "### For this tool, we are using the processing in batch, which means that the tool will process a batch of sequences on each run. The batch size is set to 100 sequences. We are submitting the batch to the site of the tool. The answer will be sent to the email address. \n",
    "\n",
    "This is basically the same code from Colab Alphafold2, just adding a function to update the csv before process, to avoid re-process the same sequence, a add a loop to iterate over the rows of the dataframe, and a function to access the drive and save the files, just in case if runtime ends, we don`t lose the progress. The following notebook was used in a Google Colab environment and copied to this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COLAB NOTEBOOK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input:\n",
    "- A `fasta_variant.csv` file\n",
    "- A `colab_alphafold2` folder\n",
    "\n",
    "Be sure that this notebook, the fasta_fariant.csv and colab_alphafold2 folder are the only files on directory\n",
    "\n",
    "## Output:\n",
    "- A .zip with all .pdb files\n",
    "\n",
    "## How to run:\n",
    "- Select `Runtime` on top menu and then `Run all`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Connect with drive\n",
    "import glob  # Import glob library for file pattern matching\n",
    "import os  # Import os library for file and directory operations\n",
    "from google.colab import drive  # Import drive module from google.colab\n",
    "\n",
    "drive.mount('/content/drive', force_remount=True)  # Mount Google Drive to the Colab environment\n",
    "filename = 'AlphaFold2.ipynb'  # Define the filename of the notebook\n",
    "drive_root = '/content/drive/MyDrive/'  # Define the root directory in Google Drive\n",
    "pattern = drive_root + '**/' + filename  # Define the pattern to search for the notebook\n",
    "file_list = glob.glob(pattern, recursive=True)  # Get the list of files matching the pattern\n",
    "notebook_dir = os.path.dirname(file_list[0])  # Get the directory of the notebook\n",
    "os.chdir(notebook_dir)  # Change the current working directory to the notebook directory\n",
    "print(f\"Add the input files on {notebook_dir}\")  # Print a message to add input files to the directory\n",
    "for filename in os.listdir():  # Iterate over the files in the directory\n",
    "    if filename != \"fasta_variant.csv\" and filename != \"colab_alphafold2\":  # Check if the file is not the input file or folder\n",
    "        file_path = os.path.join(notebook_dir, filename)  # Get the full path of the file\n",
    "        try:\n",
    "            if os.path.isfile(file_path):  # Check if it is a file\n",
    "                os.remove(file_path)  # Remove the file\n",
    "            elif os.path.isdir(file_path):  # Check if it is a directory\n",
    "                !rm -rf \"{file_path}\"  # Remove the directory\n",
    "        except Exception as e:  # Handle exceptions\n",
    "            print(f\"Failed to delete {filename}: {e}\")  # Print an error message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Setup\n",
    "%%time\n",
    "!pip install --upgrade pandas  # Upgrade pandas library\n",
    "!pip install pandarallel -U  # Upgrade pandarallel library\n",
    "import os  # Import os library for file and directory operations\n",
    "import pandas as pd  # Import pandas library for data manipulation\n",
    "import shutil  # Import shutil library for file operations\n",
    "from google.colab import files  # Import files module from google.colab\n",
    "\n",
    "from sys import version_info  # Import version_info from sys\n",
    "\n",
    "import os  # Import os library for file and directory operations\n",
    "import re  # Import re library for regular expressions\n",
    "import hashlib  # Import hashlib library for hashing\n",
    "import random  # Import random library for random operations\n",
    "PYTHON_VERSION = f\"{version_info.major}.{version_info.minor}\"  # Get the Python version\n",
    "\n",
    "if not os.path.isfile(\"COLABFOLD_READY\"):  # Check if COLABFOLD_READY file does not exist\n",
    "  print(\"installing colabfold...\")  # Print a message\n",
    "  os.system(\"pip install -q --no-warn-conflicts 'colabfold[alphafold-minus-jax] @ git+https://github.com/sokrypton/ColabFold'\")  # Install colabfold\n",
    "  if os.environ.get('TPU_NAME', False) != False:  # Check if TPU is available\n",
    "    os.system(\"pip uninstall -y jax jaxlib\")  # Uninstall jax and jaxlib\n",
    "    os.system(\"pip install --no-warn-conflicts --upgrade dm-haiku==0.0.10 'jax[cuda12_pip]'==0.3.25 -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\")  # Install specific versions of jax and dm-haiku\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/colabfold colabfold\")  # Create a symbolic link for colabfold\n",
    "  os.system(\"ln -s /usr/local/lib/python3.*/dist-packages/alphafold alphafold\")  # Create a symbolic link for alphafold\n",
    "  os.system(\"touch COLABFOLD_READY\")  # Create COLABFOLD_READY file\n",
    "\n",
    "if not os.path.isfile(\"CONDA_READY\"):  # Check if CONDA_READY file does not exist\n",
    "  print(\"installing conda...\")  # Print a message\n",
    "  os.system(\"wget -qnc https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-Linux-x86_64.sh\")  # Download Miniforge installer\n",
    "  os.system(\"bash Miniforge3-Linux-x86_64.sh -bfp /usr/local\")  # Install Miniforge\n",
    "  os.system(\"mamba config --set auto_update_conda false\")  # Configure mamba\n",
    "  os.system(\"touch CONDA_READY\")  # Create CONDA_READY file\n",
    "\n",
    "if not os.path.isfile(\"HH_READY\") and not os.path.isfile(\"AMBER_READY\"):  # Check if HH_READY and AMBER_READY files do not exist\n",
    "  print(\"installing hhsuite and amber...\")  # Print a message\n",
    "  os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")  # Install hhsuite and amber\n",
    "  os.system(\"touch HH_READY\")  # Create HH_READY file\n",
    "  os.system(\"touch AMBER_READY\")  # Create AMBER_READY file\n",
    "else:\n",
    "  if not os.path.isfile(\"HH_READY\"):  # Check if HH_READY file does not exist\n",
    "    print(\"installing hhsuite...\")  # Print a message\n",
    "    os.system(f\"mamba install -y -c conda-forge -c bioconda kalign2=2.04 hhsuite=3.3.0 python='{PYTHON_VERSION}'\")  # Install hhsuite\n",
    "    os.system(\"touch HH_READY\")  # Create HH_READY file\n",
    "  if not os.path.isfile(\"AMBER_READY\"):  # Check if AMBER_READY file does not exist\n",
    "    print(\"installing amber...\")  # Print a message\n",
    "    os.system(f\"mamba install -y -c conda-forge openmm=7.7.0 python='{PYTHON_VERSION}' pdbfixer\")  # Install amber\n",
    "    os.system(\"touch AMBER_READY\")  # Create AMBER_READY file\n",
    "\n",
    "model_type = \"auto\"  # Set model type to auto\n",
    "num_recycles = \"1\"  # Set number of recycles to 1\n",
    "recycle_early_stop_tolerance = \"auto\"  # Set recycle early stop tolerance to auto\n",
    "relax_max_iterations = 200  # Set relax max iterations to 200\n",
    "pairing_strategy = \"greedy\"  # Set pairing strategy to greedy\n",
    "calc_extra_ptm = False  # Set calc_extra_ptm to False\n",
    "max_msa = \"auto\"  # Set max_msa to auto\n",
    "num_seeds = 1  # Set number of seeds to 1\n",
    "use_dropout = False  # Set use_dropout to False\n",
    "num_recycles = int(num_recycles)  # Convert num_recycles to integer\n",
    "recycle_early_stop_tolerance = None  # Set recycle_early_stop_tolerance to None\n",
    "if max_msa == \"auto\": max_msa = None  # Set max_msa to None if it is auto\n",
    "save_all = False  # Set save_all to False\n",
    "save_recycles = False  # Set save_recycles to False\n",
    "save_to_google_drive = False  # Set save_to_google_drive to False\n",
    "dpi = 200  # Set dpi to 200\n",
    "msa_mode = \"mmseqs2_uniref_env\"  # Set msa_mode to mmseqs2_uniref_env\n",
    "pair_mode = \"unpaired_paired\"  # Set pair_mode to unpaired_paired\n",
    "\n",
    "import sys  # Import sys library\n",
    "import warnings  # Import warnings library\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)  # Ignore FutureWarnings\n",
    "from Bio import BiopythonDeprecationWarning  # Import BiopythonDeprecationWarning\n",
    "warnings.simplefilter(action='ignore', category=BiopythonDeprecationWarning)  # Ignore BiopythonDeprecationWarnings\n",
    "from pathlib import Path  # Import Path from pathlib\n",
    "from colabfold.download import download_alphafold_params, default_data_dir  # Import functions from colabfold.download\n",
    "from colabfold.utils import setup_logging  # Import setup_logging from colabfold.utils\n",
    "from colabfold.batch import get_queries, run, set_model_type  # Import functions from colabfold.batch\n",
    "from colabfold.plot import plot_msa_v2  # Import plot_msa_v2 from colabfold.plot\n",
    "\n",
    "import os  # Import os library for file and directory operations\n",
    "import numpy as np  # Import numpy library\n",
    "try:\n",
    "  K80_chk = os.popen('nvidia-smi | grep \"Tesla K80\" | wc -l').read()  # Check if Tesla K80 GPU is available\n",
    "except:\n",
    "  K80_chk = \"0\"  # Set K80_chk to \"0\" if an exception occurs\n",
    "  pass\n",
    "if \"1\" in K80_chk:  # Check if Tesla K80 GPU is available\n",
    "  print(\"WARNING: found GPU Tesla K80: limited to total length < 1000\")  # Print a warning message\n",
    "  if \"TF_FORCE_UNIFIED_MEMORY\" in os.environ:  # Check if TF_FORCE_UNIFIED_MEMORY is set\n",
    "    del os.environ[\"TF_FORCE_UNIFIED_MEMORY\"]  # Delete TF_FORCE_UNIFIED_MEMORY\n",
    "  if \"XLA_PYTHON_CLIENT_MEM_FRACTION\" in os.environ:  # Check if XLA_PYTHON_CLIENT_MEM_FRACTION is set\n",
    "    del os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"]  # Delete XLA_PYTHON_CLIENT_MEM_FRACTION\n",
    "\n",
    "from pathlib import Path  # Import Path from pathlib\n",
    "\n",
    "python_version = f\"{version_info.major}.{version_info.minor}\"  # Get the Python version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Functions\n",
    "def add_hash(x, y):\n",
    "  return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]  # Add a hash to the string\n",
    "\n",
    "def check(folder):\n",
    "  if os.path.exists(folder):  # Check if the folder exists\n",
    "    return False  # Return False if the folder exists\n",
    "  else:\n",
    "    return True  # Return True if the folder does not exist\n",
    "\n",
    "def create_repository(name, sequence):\n",
    "  python_version = f\"{version_info.major}.{version_info.minor}\"  # Get the Python version\n",
    "  query_sequence = sequence  # Set the query sequence\n",
    "  name = name  # Set the name\n",
    "  num_relax = 0  # Set the number of relaxations to 0\n",
    "  template_mode = \"pdb100\"  # Set the template mode to pdb100\n",
    "  use_amber = num_relax > 0  # Set use_amber based on num_relax\n",
    "  query_sequence = \"\".join(query_sequence.split())  # Remove whitespace from the query sequence\n",
    "\n",
    "  if not check(name):  # Check if the name is not available\n",
    "    n = 0  # Initialize n to 0\n",
    "    while not check(f\"{name}_{n}\"): n += 1  # Increment n until a unique name is found\n",
    "    name = f\"{name}_{n}\"  # Set the name to the unique name\n",
    "  os.makedirs(name, exist_ok=True)  # Create the directory\n",
    "  queries_path = os.path.join(name, f\"{name}.csv\")  # Set the path to the queries file\n",
    "  with open(queries_path, \"w\") as text_file:  # Open the queries file for writing\n",
    "    text_file.write(f\"id,sequence\\n{name},{query_sequence}\")  # Write the query sequence to the file\n",
    "\n",
    "def create_a3m_file(name, sequence):\n",
    "  a3m_file = os.path.join(name, f\"{name}.single_sequence.a3m\")  # Set the path to the a3m file\n",
    "  with open(a3m_file, \"w\") as text_file:  # Open the a3m file for writing\n",
    "    text_file.write(\">1\\n%s\" % sequence)  # Write the sequence to the file\n",
    "\n",
    "def prediction_callback(protein_obj, length, prediction_result, input_features, mode):\n",
    "  model_name, relaxed = mode  # Unpack the mode\n",
    "\n",
    "def delete_folder(folder_name):\n",
    "  \"\"\"Deletes a folder and its contents.\n",
    "\n",
    "  Args:\n",
    "      folder_name: The name of the folder to delete.\n",
    "  \"\"\"\n",
    "  if os.path.exists(folder_name):  # Check if the folder exists\n",
    "    try:\n",
    "      shutil.rmtree(folder_name)  # Remove the folder and its contents\n",
    "      print(f\"Folder '{folder_name}' deleted successfully.\")  # Print a success message\n",
    "    except OSError as e:  # Handle exceptions\n",
    "      print(f\"Error deleting folder '{folder_name}': {e}\")  # Print an error message\n",
    "  else:\n",
    "    print(f\"Folder '{folder_name}' does not exist.\")  # Print a message if the folder does not exist\n",
    "\n",
    "def run_colab_alphafold2(name, sequence):\n",
    "    filename = name.replace('_p.', '_')  # Replace '_p.' with '_' in the name\n",
    "    basejobname = \"\".join(name.split())  # Remove whitespace from the name\n",
    "    basejobname = re.sub(r'\\W+', '', basejobname)  # Remove non-alphanumeric characters from the name\n",
    "    name = add_hash(basejobname, sequence)  # Add a hash to the name\n",
    "    create_repository(name, sequence)  # Create a repository for the sequence\n",
    "    create_a3m_file(name, sequence)  # Create an a3m file for the sequence\n",
    "    queries_path = os.path.join(name, f\"{name}.csv\")  # Set the path to the queries file\n",
    "    python_version = f'{sys.version_info.major}.{sys.version_info.minor}'  # Get the Python version\n",
    "    if f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:  # Check if the site-packages directory is not in sys.path\n",
    "        sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")  # Add the site-packages directory to sys.path\n",
    "    queries, is_complex = get_queries(queries_path)  # Get the queries and check if it is a complex\n",
    "    model_type = set_model_type(is_complex, \"auto\")  # Set the model type\n",
    "    use_cluster_profile = not (\"multimer\" in model_type and max_msa is not None)  # Set use_cluster_profile based on model type and max_msa\n",
    "    download_alphafold_params(model_type, Path(\".\"))  # Download AlphaFold parameters\n",
    "    results = run(\n",
    "        queries=queries,  # Set the queries\n",
    "        result_dir=name,  # Set the result directory\n",
    "        use_templates=True,  # Set use_templates to True\n",
    "        custom_template_path=None,  # Set custom_template_path to None\n",
    "        num_relax=0,  # Set num_relax to 0\n",
    "        msa_mode=\"mmseqs2_uniref_env\",  # Set msa_mode to mmseqs2_uniref_env\n",
    "        model_type=model_type,  # Set model_type\n",
    "        num_models=1,  # Set num_models to 1\n",
    "        num_recycles=num_recycles,  # Set num_recycles\n",
    "        relax_max_iterations=relax_max_iterations,  # Set relax_max_iterations\n",
    "        recycle_early_stop_tolerance=recycle_early_stop_tolerance,  # Set recycle_early_stop_tolerance\n",
    "        num_seeds=num_seeds,  # Set num_seeds\n",
    "        use_dropout=use_dropout,  # Set use_dropout\n",
    "        model_order=[1],  # Set model_order\n",
    "        is_complex=is_complex,  # Set is_complex\n",
    "        data_dir=Path(\".\"),  # Set data_dir\n",
    "        keep_existing_results=False,  # Set keep_existing_results to False\n",
    "        rank_by=\"auto\",  # Set rank_by to auto\n",
    "        pair_mode=pair_mode,  # Set pair_mode\n",
    "        pairing_strategy=pairing_strategy,  # Set pairing_strategy\n",
    "        stop_at_score=float(100),  # Set stop_at_score\n",
    "        prediction_callback=prediction_callback,  # Set prediction_callback\n",
    "        dpi=dpi,  # Set dpi\n",
    "        zip_results=False,  # Set zip_results to False\n",
    "        save_all=save_all,  # Set save_all\n",
    "        max_msa=max_msa,  # Set max_msa\n",
    "        use_cluster_profile=use_cluster_profile,  # Set use_cluster_profile\n",
    "        input_features_callback=None,  # Set input_features_callback to None\n",
    "        save_recycles=save_recycles,  # Set save_recycles\n",
    "        user_agent=\"colabfold/google-colab-main\",  # Set user_agent\n",
    "        calc_extra_ptm=calc_extra_ptm,  # Set calc_extra_ptm\n",
    "    )\n",
    "    pdb_files = [f for f in os.listdir(name) if f.endswith('.pdb')]  # Get the list of pdb files\n",
    "    if not pdb_files:  # Check if no pdb files are found\n",
    "        print(f\"No .pdb files found in {name}\")  # Print a message\n",
    "        return\n",
    "    pdb_file = pdb_files[0]  # Get the first pdb file\n",
    "    pdb_file_path = os.path.join(name, pdb_file)  # Set the path to the pdb file\n",
    "    destination_path = os.path.join(PDB_PATH, filename + \".pdb\")  # Set the destination path for the pdb file\n",
    "    os.rename(pdb_file_path, destination_path)  # Rename the pdb file to the destination path\n",
    "    # files.download(destination_path)  # Download the pdb file\n",
    "    delete_folder(name)  # Delete the folder\n",
    "    print(f\"Model prediction saved to {destination_path}\")  # Print a success message\n",
    "\n",
    "def check_and_update_status(row):\n",
    "    variant = row[\"variant\"]  # Get the variant from the row\n",
    "    filename = variant.replace('_p.', '_')  # Replace '_p.' with '_' in the variant name\n",
    "    file_path = f\"{PDB_PATH}/{filename}.pdb\"  # Set the file path for the pdb file\n",
    "    if os.path.isfile(file_path) or row[\"colab_alphafold2\"] == 'concluded':  # Check if the pdb file exists or the status is concluded\n",
    "        return 'concluded'  # Return 'concluded'\n",
    "    return 'not_concluded'  # Return 'not_concluded'\n",
    "\n",
    "def update_colab_alphafold_status(df):\n",
    "  if 'colab_alphafold2' not in df.columns:  # Check if 'colab_alphafold2' column is not in the DataFrame\n",
    "    df['colab_alphafold2'] = 'not_concluded'  # Add 'colab_alphafold2' column with default value 'not_concluded'\n",
    "  df['colab_alphafold2'] = df.apply(check_and_update_status, axis=1)  # Update 'colab_alphafold2' status for eachdef add_hash(x, y):\n",
    "  return x + \"_\" + hashlib.sha1(y.encode()).hexdigest()[:5]  # Add a hash to the string\n",
    "\n",
    "def check(folder):\n",
    "  if os.path.exists(folder):  # Check if the folder exists\n",
    "    return False  # Return False if the folder exists\n",
    "  else:\n",
    "    return True  # Return True if the folder does not exist\n",
    "\n",
    "def create_repository(name, sequence):\n",
    "  python_version = f\"{version_info.major}.{version_info.minor}\"  # Get the Python version\n",
    "  query_sequence = sequence  # Set the query sequence\n",
    "  name = name  # Set the name\n",
    "  num_relax = 0  # Set the number of relaxations to 0\n",
    "  template_mode = \"pdb100\"  # Set the template mode to pdb100\n",
    "  use_amber = num_relax > 0  # Set use_amber based on num_relax\n",
    "  query_sequence = \"\".join(query_sequence.split())  # Remove whitespace from the query sequence\n",
    "\n",
    "  if not check(name):  # Check if the name is not available\n",
    "    n = 0  # Initialize n to 0\n",
    "    while not check(f\"{name}_{n}\"): n += 1  # Increment n until a unique name is found\n",
    "    name = f\"{name}_{n}\"  # Set the name to the unique name\n",
    "  os.makedirs(name, exist_ok=True)  # Create the directory\n",
    "  queries_path = os.path.join(name, f\"{name}.csv\")  # Set the path to the queries file\n",
    "  with open(queries_path, \"w\") as text_file:  # Open the queries file for writing\n",
    "    text_file.write(f\"id,sequence\\n{name},{query_sequence}\")  # Write the query sequence to the file\n",
    "\n",
    "def create_a3m_file(name, sequence):\n",
    "  a3m_file = os.path.join(name, f\"{name}.single_sequence.a3m\")  # Set the path to the a3m file\n",
    "  with open(a3m_file, \"w\") as text_file:  # Open the a3m file for writing\n",
    "    text_file.write(\">1\\n%s\" % sequence)  # Write the sequence to the file\n",
    "\n",
    "def prediction_callback(protein_obj, length, prediction_result, input_features, mode):\n",
    "  model_name, relaxed = mode  # Unpack the mode\n",
    "\n",
    "def delete_folder(folder_name):\n",
    "  \"\"\"Deletes a folder and its contents.\n",
    "\n",
    "  Args:\n",
    "      folder_name: The name of the folder to delete.\n",
    "  \"\"\"\n",
    "  if os.path.exists(folder_name):  # Check if the folder exists\n",
    "    try:\n",
    "      shutil.rmtree(folder_name)  # Remove the folder and its contents\n",
    "      print(f\"Folder '{folder_name}' deleted successfully.\")  # Print a success message\n",
    "    except OSError as e:  # Handle exceptions\n",
    "      print(f\"Error deleting folder '{folder_name}': {e}\")  # Print an error message\n",
    "  else:\n",
    "    print(f\"Folder '{folder_name}' does not exist.\")  # Print a message if the folder does not exist\n",
    "\n",
    "def run_colab_alphafold2(name, sequence):\n",
    "    filename = name.replace('_p.', '_')  # Replace '_p.' with '_' in the name\n",
    "    basejobname = \"\".join(name.split())  # Remove whitespace from the name\n",
    "    basejobname = re.sub(r'\\W+', '', basejobname)  # Remove non-alphanumeric characters from the name\n",
    "    name = add_hash(basejobname, sequence)  # Add a hash to the name\n",
    "    create_repository(name, sequence)  # Create a repository for the sequence\n",
    "    create_a3m_file(name, sequence)  # Create an a3m file for the sequence\n",
    "    queries_path = os.path.join(name, f\"{name}.csv\")  # Set the path to the queries file\n",
    "    python_version = f'{sys.version_info.major}.{sys.version_info.minor}'  # Get the Python version\n",
    "    if f\"/usr/local/lib/python{python_version}/site-packages/\" not in sys.path:  # Check if the site-packages directory is not in sys.path\n",
    "        sys.path.insert(0, f\"/usr/local/lib/python{python_version}/site-packages/\")  # Add the site-packages directory to sys.path\n",
    "    queries, is_complex = get_queries(queries_path)  # Get the queries and check if it is a complex\n",
    "    model_type = set_model_type(is_complex, \"auto\")  # Set the model type\n",
    "    use_cluster_profile = not (\"multimer\" in model_type and max_msa is not None)  # Set use_cluster_profile based on model type and max_msa\n",
    "    download_alphafold_params(model_type, Path(\".\"))  # Download AlphaFold parameters\n",
    "    results = run(\n",
    "        queries=queries,  # Set the queries\n",
    "        result_dir=name,  # Set the result directory\n",
    "        use_templates=True,  # Set use_templates to True\n",
    "        custom_template_path=None,  # Set custom_template_path to None\n",
    "        num_relax=0,  # Set num_relax to 0\n",
    "        msa_mode=\"mmseqs2_uniref_env\",  # Set msa_mode to mmseqs2_uniref_env\n",
    "        model_type=model_type,  # Set model_type\n",
    "        num_models=1,  # Set num_models to 1\n",
    "        num_recycles=num_recycles,  # Set num_recycles\n",
    "        relax_max_iterations=relax_max_iterations,  # Set relax_max_iterations\n",
    "        recycle_early_stop_tolerance=recycle_early_stop_tolerance,  # Set recycle_early_stop_tolerance\n",
    "        num_seeds=num_seeds,  # Set num_seeds\n",
    "        use_dropout=use_dropout,  # Set use_dropout\n",
    "        model_order=[1],  # Set model_order\n",
    "        is_complex=is_complex,  # Set is_complex\n",
    "        data_dir=Path(\".\"),  # Set data_dir\n",
    "        keep_existing_results=False,  # Set keep_existing_results to False\n",
    "        rank_by=\"auto\",  # Set rank_by to auto\n",
    "        pair_mode=pair_mode,  # Set pair_mode\n",
    "        pairing_strategy=pairing_strategy,  # Set pairing_strategy\n",
    "        stop_at_score=float(100),  # Set stop_at_score\n",
    "        prediction_callback=prediction_callback,  # Set prediction_callback\n",
    "        dpi=dpi,  # Set dpi\n",
    "        zip_results=False,  # Set zip_results to False\n",
    "        save_all=save_all,  # Set save_all\n",
    "        max_msa=max_msa,  # Set max_msa\n",
    "        use_cluster_profile=use_cluster_profile,  # Set use_cluster_profile\n",
    "        input_features_callback=None,  # Set input_features_callback to None\n",
    "        save_recycles=save_recycles,  # Set save_recycles\n",
    "        user_agent=\"colabfold/google-colab-main\",  # Set user_agent\n",
    "        calc_extra_ptm=calc_extra_ptm,  # Set calc_extra_ptm\n",
    "    )\n",
    "    pdb_files = [f for f in os.listdir(name) if f.endswith('.pdb')]  # Get the list of pdb files\n",
    "    if not pdb_files:  # Check if no pdb files are found\n",
    "        print(f\"No .pdb files found in {name}\")  # Print a message\n",
    "        return\n",
    "    pdb_file = pdb_files[0]  # Get the first pdb file\n",
    "    pdb_file_path = os.path.join(name, pdb_file)  # Set the path to the pdb file\n",
    "    destination_path = os.path.join(PDB_PATH, filename + \".pdb\")  # Set the destination path for the pdb file\n",
    "    os.rename(pdb_file_path, destination_path)  # Rename the pdb file to the destination path\n",
    "    # files.download(destination_path)  # Download the pdb file\n",
    "    delete_folder(name)  # Delete the folder\n",
    "    print(f\"Model prediction saved to {destination_path}\")  # Print a success message\n",
    "\n",
    "def check_and_update_status(row):\n",
    "    variant = row[\"variant\"]  # Get the variant from the row\n",
    "    filename = variant.replace('_p.', '_')  # Replace '_p.' with '_' in the variant name\n",
    "    file_path = f\"{PDB_PATH}/{filename}.pdb\"  # Set the file path for the pdb file\n",
    "    if os.path.isfile(file_path) or row[\"colab_alphafold2\"] == 'concluded':  # Check if the pdb file exists or the status is concluded\n",
    "        return 'concluded'  # Return 'concluded'\n",
    "    return 'not_concluded'  # Return 'not_concluded'\n",
    "\n",
    "def update_colab_alphafold_status(df):\n",
    "    if 'colab_alphafold2' not in df.columns:  # Check if 'colab_alphafold2' column is not in the DataFrame\n",
    "        df['colab_alphafold2'] = 'not_concluded'  # Add 'colab_alphafold2' column with default value 'not_concluded'\n",
    "    df['colab_alphafold2'] = df.apply(check_and_update_status, axis=1)  # Update 'colab_alphafold2' status for each row\n",
    "    print(\"Status updated based on existing files\")  # Print a status update message\n",
    "    return df  # Return the updated DataFrame\n",
    "\n",
    "def download_zip():\n",
    "    zip_filename = \"colab_alphafold2.zip\"  # Define the name of the zip file\n",
    "    zip_command = f\"zip -r {zip_filename} /content/colab_alphafold2\"  # Define the zip command\n",
    "    os.system(zip_command)  # Execute the zip command\n",
    "    files.download(zip_filename)  # Download the zip file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Colab Alphafold2..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PDB_PATH = \"colab_alphafold2\"  # Define the path to the PDB files\n",
    "if not os.path.exists(PDB_PATH):  # Check if the PDB path does not exist\n",
    "    os.makedirs(PDB_PATH)  # Create the PDB path\n",
    "variant_df = pd.read_csv('fasta_variant.csv', sep=';')  # Read the CSV file into a DataFrame\n",
    "variant_df = update_colab_alphafold_status(variant_df)  # Update the colab_alphafold2 status in the DataFrame\n",
    "# variant_df = variant_df.tail(200)  # Optionally limit the DataFrame to the last 200 rows\n",
    "variant_df.head()  # Display the first few rows of the DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variant_df['colab_alphafold2'].value_counts()  # Count the values in the colab_alphafold2 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_concluded_df = variant_df[variant_df['colab_alphafold2'] == 'not_concluded']  # Filter rows with 'not_concluded' status\n",
    "for i, (index, row) in enumerate(not_concluded_df.iterrows()):  # Iterate over the rows of the not concluded DataFrame\n",
    "    print(f\"Processing {row['variant']} ------- {i+1} of {len(not_concluded_df)}\")  # Print the progress\n",
    "    run_colab_alphafold2(row['variant'], row['fasta'])  # Run colab_alphafold2 for the variant and sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliar functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip -r colab_alphafold2.zip colab_alphafold2  # Zip the colab_alphafold2 directory\n",
    "files.download(\"colab_alphafold2.zip\")  # Download the zip file\n",
    "variant_df.to_csv('variant_df.csv', index=False, sep=';')  # Save the updated DataFrame to CSV\n",
    "files.download('variant_df.csv')  # Download the CSV file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
