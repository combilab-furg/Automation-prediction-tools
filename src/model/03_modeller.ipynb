{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeller"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to run Modeller script, you need to change the kernel to not use the virtual env once the Modeller is installed on your machine, click on the kernel on top right of this cell and chose a python kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries and loading dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob  # Import glob library for file pattern matching\n",
    "import shutil  # Import shutil library for file operations\n",
    "import pandas as pd  # Import pandas library for data manipulation\n",
    "from modeller import *  # Import all from modeller\n",
    "from modeller.automodel import *  # Import all from modeller.automodel\n",
    "from pandarallel import pandarallel  # Import pandarallel for parallel processing with pandas\n",
    "\n",
    "pandarallel.initialize()  # Initialize pandarallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = \"../../data/csv/fasta_variant.csv\"  # Define the path to the CSV file\n",
    "PDB_PATH = \"../../data/pdb/modeller\"  # Define the path to the PDB files\n",
    "ALI_PATH = \"../../data/ali\"  # Define the path to the ALI files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_pdb_file(variant):\n",
    "    pdb_files = glob.glob(os.path.join(f\"{variant}*.pdb\"))  # Get the list of PDB files for the variant\n",
    "    if not pdb_files:  # Check if no PDB files are found\n",
    "        print(f\"No PDB files found for variant {variant}\")  # Print a message\n",
    "        return\n",
    "    file_to_move = pdb_files[0]  # Get the first PDB file\n",
    "    destination_file = os.path.join(PDB_PATH, f\"{variant.replace('_p.', '_')}.pdb\")  # Define the destination file path\n",
    "    shutil.move(file_to_move, destination_file)  # Move the PDB file to the destination path\n",
    "    print(f\"Moved file: {file_to_move} to {destination_file}\")  # Print a success message\n",
    "\n",
    "def delete_other_files(variant):\n",
    "    files_to_delete = glob.glob(os.path.join(f\"{variant}*.*\"))  # Get the list of files for the variant\n",
    "    for file in files_to_delete:  # Iterate over the files\n",
    "        if not file.endswith(\".pdb\"):  # Check if the file is not a PDB file\n",
    "            os.remove(file)  # Delete the file\n",
    "            print(f\"Deleted file: {file}\")  # Print a success message\n",
    "\n",
    "def process_variant(row):\n",
    "    variant_raw = row['variant']  # Get the raw variant from the row\n",
    "    variant = variant_raw.replace(\"_p.\", \"_\")  # Replace '_p.' with '_' in the variant name\n",
    "    ali_file = os.path.join(ALI_PATH, f\"{variant}.ali\")  # Define the path to the ALI file\n",
    "    if os.path.exists(ali_file):  # Check if the ALI file exists\n",
    "        with open(ali_file, \"r\") as file:  # Open the ALI file for reading\n",
    "            file_content = file.read()  # Read the ALI content\n",
    "\n",
    "        headers = [line for line in file_content.split('\\n') if line.startswith('>')]  # Get the headers from the ALI content\n",
    "        gene = headers[0].split(';')[1]  # Get the gene from the headers\n",
    "\n",
    "        env = Environ()  # Create an Environ object\n",
    "        env.io.atom_files_directory = ['.', 'data/pdb/blast']  # Set the atom files directory\n",
    "\n",
    "        a = AutoModel(env,\n",
    "                      alnfile=ali_file,  # Alignment filename\n",
    "                      knowns=gene,  # Codes of the templates\n",
    "                      sequence=variant_raw)  # Code of the target\n",
    "        a.starting_model = 1  # Index of the first model\n",
    "        a.ending_model = 1  # Index of the last model\n",
    "                                           \n",
    "        a.make()  # Make the model\n",
    "\n",
    "        move_pdb_file(variant_raw)  # Move the PDB file\n",
    "        delete_other_files(variant_raw)  # Delete other files\n",
    "\n",
    "        row['modeller'] = 'concluded'  # Update the status to 'concluded'\n",
    "    else:\n",
    "        print(f\"ALI file for variant {variant} not found.\")  # Print a message if the ALI file is not found\n",
    "    return row  # Return the updated row\n",
    "\n",
    "def check_and_update_status(row):\n",
    "    variant = row[\"variant\"]  # Get the variant from the row\n",
    "    filename = variant.replace('_p.', '_')  # Replace '_p.' with '_' in the variant name\n",
    "    file_path = f\"{PDB_PATH}/{filename}.pdb\"  # Define the file path for the PDB file\n",
    "    print(file_path)  # Print the file path\n",
    "    if os.path.isfile(file_path):  # Check if the PDB file exists\n",
    "        return 'concluded'  # Return 'concluded' if the file exists\n",
    "    return 'not_concluded'  # Return 'not_concluded' if the file does not exist\n",
    "\n",
    "def update_status(df, column):\n",
    "    if column not in df.columns:  # Check if the column is not in the DataFrame\n",
    "        df[column] = 'not_concluded'  # Add the column with default value 'not_concluded'\n",
    "    df[column] = df.apply(check_and_update_status, axis=1)  # Update the status for each row\n",
    "    print(\"Status updated based on existing files\")  # Print a status update message\n",
    "    return df  # Return the updated DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "variant_df = pd.read_csv(CSV_PATH, sep=';')  # Read the CSV file into a DataFrame\n",
    "\n",
    "variant_df = update_status(variant_df, 'modeller')  # Update the modeller status in the DataFrame\n",
    "\n",
    "print(f\"Total count: {len(variant_df)}\")  # Print the total count of rows\n",
    "print(variant_df['modeller'].value_counts())  # Count the values in the modeller column\n",
    "input(\"\\nPress Enter to continue...\")  # Wait for user input\n",
    "not_concluded_df = variant_df[variant_df['modeller'] == 'not_concluded']  # Filter rows with 'not_concluded' status\n",
    "\n",
    "not_concluded_df = not_concluded_df.parallel_apply(process_variant, axis=1)  # Apply the process_variant function in parallel\n",
    "input()  # Wait for user input\n",
    "variant_df.update(not_concluded_df)  # Update the original DataFrame with the processed rows\n",
    "variant_df.to_csv(CSV_PATH, sep=';', index=False)  # Save the updated DataFrame to CSV\n",
    "\n",
    "print(f\"\\n\\nTotal count: {len(variant_df)}\")  # Print the total count of rows\n",
    "print(variant_df['modeller'].value_counts())  # Count the values in the modeller column"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
